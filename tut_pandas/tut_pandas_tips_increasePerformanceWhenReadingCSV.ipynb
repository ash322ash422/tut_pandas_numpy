{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e51c23-e56a-4b95-92c9-981a23cae3ab",
   "metadata": {},
   "source": [
    "# Pandas: How to read the data files the right way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7371ccb4",
   "metadata": {},
   "source": [
    "### Lets generate data_file that we would use to test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef393cb9-4a96-46b8-ae85-eaee199908a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hi\\AppData\\Local\\Temp\\ipykernel_5632\\1893042939.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix # for creating sparse data\n",
    "\n",
    "data_file = 'DELETE_ME_huge_file.csv' # Do NOT FORGET TO DELETE THIS AT THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb39826f-aeab-460f-acff-8ff7605a3a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset 'DELETE_ME_huge_file.csv' with 1000000 rows created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Lets generate some fake data and save to a csv file for testing:\n",
    "# NOTE: Took 3 sec to generate the CSV file\n",
    "\n",
    "n = 1_000_000 # Number of rows: 1 million\n",
    "\n",
    "np.random.seed(42) # seed for reproducibility\n",
    "\n",
    "# Generate data\n",
    "data = {\n",
    "    \"id\"     : np.arange(1, n + 1),\n",
    "    \"name\"   : np.random.choice([\"Fernando\", \"Prakash\", \"Shamlodhiya\", \"Smith\", \"Patel\", \"Juno\"], size=n),\n",
    "    \"age\"    : np.random.randint(20, 80, size=n),\n",
    "    \"amount\" : np.round(np.random.uniform(50, 500, size=n), 2),\n",
    "    \"status\" : np.random.choice([\"Paid\", \"Pending\", \"Failed\"], size=n, p=[0.6, 0.3, 0.1])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv(data_file, index=False) # Save as CSV\n",
    "\n",
    "print(f\"Sample dataset '{data_file}' with {n} rows created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eeea6e7d-10e1-4274-8eff-04a4524febc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id         name  age  amount   status\n",
      "758368  758369         Juno   76  128.56  Pending\n",
      "537469  537470  Shamlodhiya   63  185.86  Pending\n",
      "848667  848668        Smith   21  333.20     Paid\n",
      "426683  426684  Shamlodhiya   56  134.34  Pending\n",
      "128391  128392      Prakash   39  347.50     Paid\n",
      "966893  966894         Juno   65  436.60     Paid\n",
      "192879  192880     Fernando   57  438.13     Paid\n",
      "530489  530490        Patel   54  454.59     Paid\n",
      "847744  847745     Fernando   20  231.74     Paid\n",
      "858672  858673     Fernando   30  413.73     Paid\n"
     ]
    }
   ],
   "source": [
    "# lets look at few values\n",
    "print(df.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c11cae-cc75-4937-bf73-bde40a59685c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142f583-26cb-46fb-a7e5-ea0cad249ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3e4a3ec-0efe-4ef0-9b35-5581f7a2a352",
   "metadata": {},
   "source": [
    "## 1. Use dtype to specify column types\n",
    "- Avoid pandas guessing data. **Define dtypes for each column during reading of file**\n",
    "- The **process is slower but is memory efficient**...takes space in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "693fa718-0549-47e8-9d52-7e4d30abdb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id         name  age  amount   status\n",
      "0   1        Smith   78   58.66  Pending\n",
      "1   2        Patel   71  207.34     Paid\n",
      "2   3  Shamlodhiya   40   66.54     Paid\n",
      "3   4        Patel   32  326.49  Pending\n",
      "4   5        Patel   49  380.40     Paid\n",
      "Time in sec: 0.390625\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   id      1000000 non-null  int64  \n",
      " 1   name    1000000 non-null  object \n",
      " 2   age     1000000 non-null  int64  \n",
      " 3   amount  1000000 non-null  float64\n",
      " 4   status  1000000 non-null  object \n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 142.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# step1: Without specifying dtypes\n",
    "# NOTE: Took 0.4 secs\n",
    "\n",
    "start = time.process_time()\n",
    "df    = pd.read_csv(data_file)\n",
    "end   = time.process_time()\n",
    "\n",
    "print(df.head())\n",
    "print(\"Time in sec:\",end - start)\n",
    "print(df.info(memory_usage=\"deep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16538a4c-e485-4f78-ae86-289b98717708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id         name  age      amount   status\n",
      "0   1        Smith   78   58.660000  Pending\n",
      "1   2        Patel   71  207.339996     Paid\n",
      "2   3  Shamlodhiya   40   66.540001     Paid\n",
      "3   4        Patel   32  326.489990  Pending\n",
      "4   5        Patel   49  380.399994     Paid\n",
      "Time in sec: 0.4375\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count    Dtype   \n",
      "---  ------  --------------    -----   \n",
      " 0   id      1000000 non-null  int32   \n",
      " 1   name    1000000 non-null  string  \n",
      " 2   age     1000000 non-null  uint8   \n",
      " 3   amount  1000000 non-null  float32 \n",
      " 4   status  1000000 non-null  category\n",
      "dtypes: category(1), float32(1), int32(1), string(1), uint8(1)\n",
      "memory usage: 70.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# step2: with dtype explicitly mentioned: \n",
    "# NOTE: Took 4.2 secs\n",
    "\n",
    "dtypes = {\n",
    "    'id'    : 'int32', \n",
    "    'name'  : 'category', # Note: if names repeat many times, then use 'category', else use 'string'\n",
    "    'age'   : 'uint8',    # this num is going to be 0 - 120. So use uint8\n",
    "    'amount': 'float32',\n",
    "    'status': 'category'\n",
    "}\n",
    "start = time.process_time()\n",
    "df = pd.read_csv(data_file, dtype=dtypes)\n",
    "end = time.process_time()\n",
    "\n",
    "print(df.head())\n",
    "print(\"Time in sec:\",end - start)\n",
    "print(df.info(memory_usage=\"deep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50a63d0-ede8-4fbb-ac1d-051ec3daa468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much memory saved? 92%\n",
    "(143-11)/143"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0acb54-db9e-4230-8077-951afacaa3d2",
   "metadata": {},
   "source": [
    "## conclusion: The process slows down little bit, but you save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69809f43-378a-49a2-ae10-f621a28d9406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596e828-9747-49eb-b147-14447312b84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6b9c0f5-7ade-43eb-b0d6-77db3f12af40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572e55f-6d32-4444-97a3-b2c53a4f51fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b37fa-7ea3-40f3-aa95-6130fa988c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f93fb-d72f-404a-90a7-d6ffa0a24e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c57adc5-1bca-46d6-a9b5-9172b4a41294",
   "metadata": {},
   "source": [
    "## 2. Load only required columns: \n",
    "- Make use of **usecols**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3dda149-9c24-493a-a519-d3ea49c481bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id         name  age  amount   status\n",
      "0   1        Smith   78   58.66  Pending\n",
      "1   2        Patel   71  207.34     Paid\n",
      "2   3  Shamlodhiya   40   66.54     Paid\n",
      "3   4        Patel   32  326.49  Pending\n",
      "4   5        Patel   49  380.40     Paid\n",
      "0.296875\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   id      1000000 non-null  int64  \n",
      " 1   name    1000000 non-null  object \n",
      " 2   age     1000000 non-null  int64  \n",
      " 3   amount  1000000 non-null  float64\n",
      " 4   status  1000000 non-null  object \n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 142.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# step1: Read all columns: The hard way\n",
    "\n",
    "start = time.process_time()\n",
    "df = pd.read_csv(data_file)\n",
    "end = time.process_time()\n",
    "\n",
    "print(df.head())\n",
    "print(end - start)\n",
    "print(df.info(memory_usage=\"deep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "992c3d85-939d-4a38-b1fc-4f1d771ca25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  amount\n",
      "0   78   58.66\n",
      "1   71  207.34\n",
      "2   40   66.54\n",
      "3   32  326.49\n",
      "4   49  380.40\n",
      "0.3125\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   age     1000000 non-null  int64  \n",
      " 1   amount  1000000 non-null  float64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 15.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# step2: Read only required columns\n",
    "\n",
    "start = time.process_time()\n",
    "df = pd.read_csv(data_file, usecols=['age', 'amount'])\n",
    "end = time.process_time()\n",
    "\n",
    "print(df.head())\n",
    "print(end - start)\n",
    "print(df.info(memory_usage=\"deep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df78900-4b28-4969-961f-43e39327b40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8922535211267606"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much saving in memory: 89%\n",
    "(142-15.3)/142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f34873-cb16-44b7-bef7-24dd2795d0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc72801-c19f-4bef-a176-3073ed782463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54224f-20c8-4923-93d5-472109cb7768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466f4afe-907c-4bb6-95eb-028a8795bcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2cabcc-1190-4da4-a20a-3d4a40b153e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69e8b347-d152-4f8a-a7c8-33a433e47e14",
   "metadata": {},
   "source": [
    "### 3. Sparse data\n",
    "- Most values are the same (often zeros or NaNs) and only a small fraction are **useful** non-zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18431296-757f-452a-89a6-1f2562c334f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset 'DELETE_ME_huge_file.csv' with 100000 rows created successfully!\n"
     ]
    }
   ],
   "source": [
    "# step1: Lets generate some fake data and save to a csv file for testing:\n",
    "\n",
    "n = 100000 # Number of rows\n",
    "\n",
    "np.random.seed(42) # seed for reproducibility\n",
    "\n",
    "sparsity = 0.9  \n",
    "values = np.random.randint(1, 10, size=n) # Random values\n",
    "mask = np.random.choice([0, 1], size=n, p=[sparsity, 1-sparsity]) # Mask for sparsity\n",
    "sparse_data = values * mask\n",
    "sparse_vec = csr_matrix(sparse_data) # Convert to sparse vector\n",
    "\n",
    "# Generate data\n",
    "data = {\n",
    "    \"id\"          : np.arange(1, n + 1),\n",
    "    \"commute_dist\": sparse_vec.toarray().ravel()\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv(data_file, index=False) # Save as CSV\n",
    "\n",
    "print(f\"Sample dataset '{data_file}' with {n} rows created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "558ba0b9-ade9-4dd6-9453-2e851addfe1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  commute_dist\n",
      "10761  10762             0\n",
      "32648  32649             0\n",
      "92274  92275             0\n",
      "19648  19649             0\n",
      "12561  12562             0\n",
      "72694  72695             0\n",
      "18386  18387             0\n",
      "84271  84272             0\n",
      "73197  73198             0\n",
      "65941  65942             0\n",
      "commute_dist\n",
      "0    90031\n",
      "6     1147\n",
      "2     1143\n",
      "7     1127\n",
      "8     1118\n",
      "5     1100\n",
      "9     1097\n",
      "4     1087\n",
      "3     1081\n",
      "1     1069\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Lets see the data for column commute_dist\n",
    "print(df.sample(10))\n",
    "print(df['commute_dist'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0686f929-5bd3-45b1-8e95-3a159493199a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense DataFrame memory usage:\n",
      "Index              128\n",
      "id              800000\n",
      "commute_dist    800000\n",
      "dtype: int64\n",
      "Total: 1600128 bytes\n"
     ]
    }
   ],
   "source": [
    "# step2: read the CSV file the hard way: Without specifying sparse datatype\n",
    "\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "print(\"Dense DataFrame memory usage:\")\n",
    "print(df.memory_usage(deep=True))\n",
    "print(\"Total:\", df.memory_usage(deep=True).sum(), \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a291766-7f21-498f-810f-c83be294d8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparse DataFrame memory usage:\n",
      "Index              128\n",
      "id              800000\n",
      "commute_dist     79752\n",
      "dtype: int64\n",
      "Total: 879880 bytes\n"
     ]
    }
   ],
   "source": [
    "# step3: convert to pandas SparseDtype\n",
    "\n",
    "df_sparse = df.copy()\n",
    "df_sparse[\"commute_dist\"] = df_sparse[\"commute_dist\"].astype(\"Sparse[int]\")\n",
    "\n",
    "print(\"\\nSparse DataFrame memory usage:\")\n",
    "print(df_sparse.memory_usage(deep=True))\n",
    "print(\"Total:\", df_sparse.memory_usage(deep=True).sum(), \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3af055e-36c6-4d88-8de7-1c8958742dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90031"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# savings : 90%\n",
    "(800000-79752)/800000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66573e33-b7d3-45b1-b4eb-43d65601c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36cc2b-df5a-4910-a469-156528ddf117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3d7ed-b824-423f-85fb-cc0a0752dc44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b6d2d-bc99-463a-8933-496d88b2a02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35141fb-4cbf-4601-be39-a5fb15238368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07ed7e-a5b9-4007-86e3-cd5c98cd34af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6250867a-02af-450c-9720-f696757eea0b",
   "metadata": {},
   "source": [
    "## Load Boolean type\n",
    "- If we have columns that are boolean in nature, then use dataype boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40294617-2c1b-4a7b-9661-3a8b4818ec4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset 'DELETE_ME_huge_file.csv' with 1000000 rows created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Dealing boolean data type\n",
    "# step1: Lets generate some fake data and save to a csv file for testing:\n",
    "\n",
    "n = 1_000_000 # Number of rows: 1 million\n",
    "\n",
    "np.random.seed(42) # seed for reproducibility\n",
    "\n",
    "# Generate data\n",
    "data = {\n",
    "    \"employee_id\" : np.arange(1, n + 1),\n",
    "    \"is_working\"  : np.random.choice([\"Not working\", \"Working\"], size=n), # only 2 string choice\n",
    "    \"is_remote\"  : np.random.choice([0, 1], p=[0.9, 0.1], size=n),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(data_file, index=False) # Save as CSV\n",
    "print(f\"Sample dataset '{data_file}' with {n} rows created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "667e4e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id   is_working  is_remote\n",
      "0            1  Not working          0\n",
      "1            2      Working          0\n",
      "2            3  Not working          0\n",
      "3            4  Not working          1\n",
      "4            5  Not working          0\n"
     ]
    }
   ],
   "source": [
    "# lets see few records\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d89589e8-23d6-4992-ac96-b9ce566b9abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   employee_id  1000000 non-null  int64 \n",
      " 1   is_working   1000000 non-null  object\n",
      " 2   is_remote    1000000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 70.6 MB\n",
      "None \n",
      "\n",
      "Index               132\n",
      "employee_id     8000000\n",
      "is_working     58000756\n",
      "is_remote       8000000\n",
      "dtype: int64 \n",
      "\n",
      "Total: 74000888 bytes\n"
     ]
    }
   ],
   "source": [
    "# step2: load file without dtypes\n",
    "\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "# print memory stats\n",
    "print(df.info(memory_usage=\"deep\"),\"\\n\")\n",
    "print(df.memory_usage(deep=True),\"\\n\")\n",
    "print(\"Total:\", df.memory_usage(deep=True).sum(), \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75d71368-d90f-446e-bc2c-42e3ae6066a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count    Dtype   \n",
      "---  ------       --------------    -----   \n",
      " 0   employee_id  1000000 non-null  int32   \n",
      " 1   is_working   1000000 non-null  category\n",
      " 2   is_remote    1000000 non-null  bool    \n",
      "dtypes: bool(1), category(1), int32(1)\n",
      "memory usage: 5.7 MB\n",
      "None \n",
      "\n",
      "Index              132\n",
      "employee_id    4000000\n",
      "is_working     1000116\n",
      "is_remote      1000000\n",
      "dtype: int64 \n",
      "\n",
      "Total: 6000248 bytes\n"
     ]
    }
   ],
   "source": [
    "# step3: Explicitly set dtypes\n",
    "dtypes = {\n",
    "    \"employee_id\": \"int32\",      # 1M fits in int32 (max 2.1B)\n",
    "    \"is_working\": \"category\",    # only 2 unique values → best as category\n",
    "    \"is_remote\": \"bool\"          # NOTE: use 'bool' if column has no missing values, else use 'boolean'\n",
    "}\n",
    "\n",
    "df = pd.read_csv(data_file, dtype=dtypes)\n",
    "\n",
    "# print memory stats\n",
    "print(df.info(memory_usage=\"deep\"),\"\\n\")\n",
    "print(df.memory_usage(deep=True),\"\\n\")\n",
    "print(\"Total:\", df.memory_usage(deep=True).sum(), \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3151b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id   is_working  is_remote\n",
      "0            1  Not working      False\n",
      "1            2      Working      False\n",
      "2            3  Not working      False\n",
      "3            4  Not working       True\n",
      "4            5  Not working      False\n"
     ]
    }
   ],
   "source": [
    "# Lets look at few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88bcff87-4361-4807-8c9b-2ef83ae2b834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827568454452559\n",
      "0.875\n"
     ]
    }
   ],
   "source": [
    "# how much saving for column is_working: 98%\n",
    "print((58000756 -  1000116)/58000756)\n",
    "\n",
    "# how much saving for column is_remote: 87%\n",
    "print((8000000 - 1000000)/8000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e11a6a54-2f82-476f-9537-2e1c9a2f6858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count    Dtype\n",
      "---  ------       --------------    -----\n",
      " 0   employee_id  1000000 non-null  int32\n",
      " 1   is_working   1000000 non-null  bool \n",
      " 2   is_remote    1000000 non-null  bool \n",
      "dtypes: bool(2), int32(1)\n",
      "memory usage: 5.7 MB\n",
      "None \n",
      "\n",
      "Index              132\n",
      "employee_id    4000000\n",
      "is_working     1000000\n",
      "is_remote      1000000\n",
      "dtype: int64 \n",
      "\n",
      "Total: 6000132 bytes\n"
     ]
    }
   ],
   "source": [
    "# step4: (OPTIONAL)  for column is_working, map \"Yes\"->True, \"No\"->False and change the dtype to bool\n",
    "\n",
    "# Convert Yes/No → True/False\n",
    "df[\"is_working\"] = df[\"is_working\"].map({\"Working\": True, \"Not working\": False}).astype(\"bool\")\n",
    "\n",
    "print(df.info(memory_usage=\"deep\"),\"\\n\")\n",
    "print(df.memory_usage(deep=True),\"\\n\")\n",
    "print(\"Total:\", df.memory_usage(deep=True).sum(), \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f903bf5-3997-44c4-b01d-967c3c6d4157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00011598654556071496"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much saving for is_working: 0.01% (Not significant here)\n",
    "(1000116 - 1000000)/1000116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a9d7b-58b4-4647-b537-7a2a0bada044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35feabb7-5237-4a7e-baee-50b5b1ff4525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f039e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8656377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1cb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba30a4b-3201-48d2-9545-1d443149c44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b07ea8f-a2a7-40c4-9bdc-04c4076a8717",
   "metadata": {},
   "source": [
    "## 4. Process large files in chunks using chunksize\n",
    "\n",
    "### When to use chunking ?\n",
    "\n",
    "You only need to aggregate results (e.g., sum, mean, counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac668a4-d17c-4136-8085-057aa03afe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a large CSV (5 million rows) # took 30 sec\n",
    "rows = 50_000_000\n",
    "df = pd.DataFrame({\n",
    "    \"id\": np.arange(rows),\n",
    "    \"value\": np.random.randint(0, 100, size=rows)\n",
    "})\n",
    "\n",
    "df.to_csv(data_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c66f7e03-770f-43fd-a00c-212ad69cf9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full load:    sum=2475237176, time=4.78 sec\n"
     ]
    }
   ],
   "source": [
    "# 2. Read at once (memory heavy)\n",
    "start   = time.process_time()\n",
    "df_full = pd.read_csv(data_file)\n",
    "total_sum_full = df_full[\"value\"].sum()\n",
    "end     = time.process_time()\n",
    "print(f\"Full load:    sum={total_sum_full}, time={end - start:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4c2bb26-902b-4e02-849b-eeb1a2d299eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked load: sum=2475237176, time=7.31 sec\n"
     ]
    }
   ],
   "source": [
    "# 3. Read in chunks (memory light)\n",
    "start = time.process_time()\n",
    "total_sum_chunk = 0\n",
    "chunks = pd.read_csv(data_file, chunksize=100_000)  # 100k rows at a time\n",
    "for chunk in chunks:\n",
    "    total_sum_chunk += chunk[\"value\"].sum()\n",
    "end = time.process_time()\n",
    "print(f\"Chunked load: sum={total_sum_chunk}, time={end - start:.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d1209-d036-4195-867a-51abf0d098e7",
   "metadata": {},
   "source": [
    "## conclusion\n",
    "- Both methods give the same result\n",
    "- Chunking uses far less memory (you’re only holding 100k rows in memory at any given time instead of 5 million).\n",
    "- The speed may be slightly better or slightly worse depending on system I/O, but the **real win is memory efficiency**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e6352-07f0-4052-94bb-2153067e5622",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b37f2aed-c6e3-4651-9bf2-8153d1845f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked load: mean=49.49528002, time=6.75 sec\n"
     ]
    }
   ],
   "source": [
    "# Now lets calculate mean value using chunking: took 12 sec\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "total_sum   = 0\n",
    "total_count = 0\n",
    "\n",
    "chunks = pd.read_csv(data_file, chunksize=100_000)  # 100k rows at a time\n",
    "for chunk in chunks:\n",
    "    total_sum   += chunk[\"value\"].sum()\n",
    "    total_count += chunk[\"value\"].count()   # count non-NaN values\n",
    "\n",
    "mean_value = total_sum / total_count\n",
    "\n",
    "end = time.process_time()\n",
    "\n",
    "print(f\"Chunked load: mean={mean_value}, time={end - start:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e308f-939c-470e-9bec-7bd54648fa4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d5551-12e0-4c4f-a08b-c7db208c1872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed5f4f-d60a-49c6-ac5a-f7a93f153734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a8968-f70f-4000-9b32-5c3cb9db0434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a476b976-a9cf-4402-9a47-e82e29dce310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f55c4d-2d69-4dba-8d72-d1860d512da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c954f36-bc99-45f7-88eb-4e1c4781abeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7610bf-951c-4bc7-b8d1-a92dd37b2be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74fafe97-2c7b-4483-97ef-70545ba8e137",
   "metadata": {},
   "source": [
    "## Use compression if reading from zipped files\n",
    "### Why compression helps\n",
    "\n",
    "- Smaller file size on disk and reduces storage costs\n",
    "- A 1 GB CSV can shrink to ~200 MB (or less) with gzip.\n",
    "- Faster reads (despite decompression)\n",
    "- Reading from disk/network is often the slowest part (I/O bottleneck). **If the file is compressed, you transfer/read fewer bytes.**\n",
    "\n",
    "## Test it if it improves performance. In my case it did NOT\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dd06ebd-6d12-4797-a4c6-c859aa81d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# step 1) generate fake data. (Note: This took 10 sec)\n",
    "n = 1_000_000 # Generate dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "data = {\n",
    "    \"employee_id\": np.arange(1, n + 1),\n",
    "    \"is_working\" : np.random.choice([\"Yes\", \"No\"], size=n),\n",
    "    \"is_remote\"  : np.random.choice([0, 1], p=[0.9, 0.1], size=n),\n",
    "    \"value\"      : np.random.randn(n) * 100  # extra numeric column\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "csv_file = \"DELETE_big_data.csv\" # Save uncompressed CSV\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "csv_gz_file = \"DELETE_big_data.csv.gz\" # Save compressed CSV (gzip)\n",
    "df.to_csv(csv_gz_file, index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b661786-a61b-42db-a900-65942e051ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size (uncompressed): 30.78 MB\n",
      "File size (gzip):         12.20 MB\n"
     ]
    }
   ],
   "source": [
    "# step2) Compare file sizes\n",
    "\n",
    "size_uncompressed = os.path.getsize(csv_file) / (1024**2)\n",
    "size_compressed = os.path.getsize(csv_gz_file) / (1024**2)\n",
    "print(f\"File size (uncompressed): {size_uncompressed:.2f} MB\")\n",
    "print(f\"File size (gzip):         {size_compressed:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44288b9b-ace9-49f8-b996-e46ed8bd9c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read DELETE_big_data.csv | Time: 0.28 sec | Memory: 79.63 MB\n",
      "Read DELETE_big_data.csv.gz | Time: 0.56 sec | Memory: 79.63 MB\n"
     ]
    }
   ],
   "source": [
    "# 3. Benchmark read times + memory\n",
    "\n",
    "start = time.process_time()\n",
    "df = pd.read_csv(csv_file)\n",
    "end = time.process_time()\n",
    "mem = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f\"Read {csv_file:<15} | Time: {end - start:.2f} sec | Memory: {mem:.2f} MB\")\n",
    "\n",
    "\n",
    "start = time.process_time()\n",
    "df = pd.read_csv(DELETE_big_data, compression=\"gzip\")\n",
    "end = time.process_time()\n",
    "mem = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f\"Read {csv_gz_file:<15} | Time: {end - start:.2f} sec | Memory: {mem:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca3b85e-4fe7-4663-b917-0f1c964711e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b9d3b-9e2c-4a07-a2c7-42641830b3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be320659-14f0-416d-ab50-57e97f16c4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad627d7-23e3-438a-a349-9f69f48ce2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674298d2-21a9-4fdf-b6bc-ad26117fbdf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea377d7-cefb-4cf1-b3c8-78debc62207b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b555fba-1823-4d90-b852-276851b35af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4329baa-e9d9-47a5-8216-b7d0eeed4ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c9c96-7609-4695-8994-c426bcb87e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbebb1f-70ff-409e-b284-ec31771566b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48c75d-69db-4b9b-a2ad-056e5d14f350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8b399-d399-43d0-a95a-700cea621e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. Parse dates efficiently using parse_dates\n",
    "df = pd.read_csv(data_file, parse_dates=['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613bd51-701c-4f19-9b93-f955c13e76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP (Delete the data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8187aef-7d0a-4a01-b6f5-66a6fe1b3d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1866a91-f19c-4ac6-be55-4897baa6654f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8b3e1-c22b-4349-9083-a32b4890f89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe06014-3a2c-4a2d-930a-2663cdfac213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b52e700-10af-479f-a19a-c4f353839686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919906de-ed09-4630-8f4a-6ea7a638791f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f06d07fd-0d41-45ea-9e6e-c899c336e00e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pyarrow'.  Use pip or conda to install pyarrow.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:984\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ## 8. Use faster backend engines (pandas 2.0+)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Alternative to engine='c'\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1012\u001b[0m     dialect,\n\u001b[0;32m   1013\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:624\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1909\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1907\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1908\u001b[0m         \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m-> 1909\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1910\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1911\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\arrow_parser_wrapper.py:244\u001b[0m, in \u001b[0;36mArrowParserWrapper.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m    234\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    Reads the contents of a CSV file into a DataFrame and\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m    processes it according to the kwargs passed in the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m        The DataFrame created from the CSV file.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m     pa \u001b[38;5;241m=\u001b[39m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     pyarrow_csv \u001b[38;5;241m=\u001b[39m import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pyarrow_options()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'pyarrow'.  Use pip or conda to install pyarrow."
     ]
    }
   ],
   "source": [
    "# ## 8. Use faster backend engines (pandas 2.0+)\n",
    "# df = pd.read_csv(data_file, engine='pyarrow')  # Alternative to engine='c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c1592-5705-4e7d-ac63-a9fda31fe759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
